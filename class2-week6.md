# 改善深层神经网络：超参数调试、正则化以及优化


## Week2 优化算法



2.1  Mini-batch 梯度下降法

### 2.2  理解 mini-batch 梯度下降法

`mini-batch size`
- `size = m` 批次梯度下降
    - 下降比较平缓和快
    - 每一个迭代训练时间比较长
- `size = 1` 随机梯度下降
    - 下降噪声会比较大
    - 失去利用向量加速
    
- 所以选择了一个中间值作为`mini-batch size`








2.3  指数加权平均

2.4  理解指数加权平均

2.5  指数加权平均的偏差修正

2.6  动量梯度下降法

2.7  RMSprop

2.8  Adam 优化算法

2.9  学习率衰减

2.10  局部最优的问题



